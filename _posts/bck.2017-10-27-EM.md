---
layout: post
title: Finite Mixture Models and the EM-Algorithm
date: 2017-10-27 17:33
author: baruuum
comments: true
categories: [EM Algorithm, Mixture Modeling, MLE, Quant Stuff]
---
Today, I was somewhat annoyed by the fact that

1. I've read about, and thought that I've understood, the EM-algorithm a long time ago,
2. I've been using the algorithm for many appliations (of course, via pre-written programs)
3. but, once I've tried to program my own algorithm for a new application, I was not able to do so.

Thus, in this post, I'll try to go step-by-step through the theory and implementation of the algorithm, hopefully, once and for all. I have a very bad feeling that this will become a looooong post ...

<h3>Mixture Models</h3>

The EM algorithm is often used for maximum likelihood estimation when we have incomplete data, by which I mean that we have either missing values or unobserved variables. While the EM algorithm is very general and, thus, can be used in many maximization problems, it is most often used for mixture modeling, where the unobserved variable is categorical. Let us take a simple example. Suppose that we are trying to model an observed outcome $ Y\sim f $. Suppose that theory predicts that this outcome is generated by the a two-step process: 

1. There exists $ K $ classes in the population, which are unfortunately unobserved and represented by the class-membership variable $ Z $, and 
2. for each class $ 1\le j \le K $, the outcome follows the distribution $ f_j=f(\cdot \,\vert\,  Z=j) $. 

In other words, we might say that

$$  Z \sim \text{Categorical}({\lambda})  $$

and

$$  Y\,\vert\, Z=j \sim f_j.  $$

where the $ \lambda=\{\lambda_1,...,\lambda_K\} $ is the vector of parameters which reflect the the probability that a randomly drawn observation comes from class $ 1\le j\le K $, so that $ \mathbf 1'{\lambda} = \sum_j \lambda_j=1 $. Thus, $ \lambda_j $ can be understood as the relative size of class $ j $.

Models that have these structure are often referred to as **finite mixture models**, finite because there are a finite number of mixture components (i.e., the classes). So far, the distributions $ f $ and $ f_j $, $ j=1,...,K $, have not been given a parametric form. To do so, we might assume that $ f_j $ is characterized by a parameter vector $ \theta_j $. For example, if $ f_j $ is assumed to be the Normal density, we would write $ f_j(\cdot \,\vert\,  \theta_j)=\text{Normal}(\cdot \,\vert\, \theta_j) $, where the parameter vector would be $ \theta_j=(\mu_j,\sigma_j) $ with $ \mu_j $ and $ \sigma_j $ being, respectively, the mean and standard deviation of the distribution. This would give $ Z\sim \text{Categorical}(\lambda_1,...,\lambda_K) $ and $ f(y\,\vert\, Z=j)=f_j(y\,\vert\, \theta_j) = \text{Normal}(y\,\vert\, \mu_j,\sigma_j) $.

Collecting these parameter vectors and the mixing parameters into a parmeter vector for the whole model, we have $ \theta=(\lambda,\theta_1, \theta_1,...,\theta_K) $, which will be the estimand of our analysis.

<h3>Likelihood: Three Distributions</h3>

Now, we return to mixture models and their estimation. Let $ Z $ be the class membership variable with $ j=1,2,...,K $ categories, and assume that $ Y\,\vert\, Z=j \sim f_j(\cdot\,\vert\, \theta_j) $. Let us define three distributions.

First, let $ g(\cdot,\cdot\,\vert\, \theta) $ be the joint distribution of $ Y $ and $ Z $. Assuming a random sample of size $ n $, $ g $ can be expressed as

$$  g(\mathbf y,\mathbf z\,\vert\, \theta) = \prod_{i=1}^n g(y_i,z_i\,\vert\, \theta) = \prod_{i=1}^n f(y_i\,\vert\, z_i,\theta)p(z_i\,\vert\, \theta)=\prod_{i=1}^n \prod_{j=1}^K \big[f_j(y_i\,\vert\, \theta_j)\lambda_j\big]^{\mathbb I(z_i=j)},  $$

where $ \mathbf y = (y_1,...,y_n), \mathbf z = (z_1,...,z_n), \mathbb I(\cdot) $ is an indicator function, and $ p(z_i\,\vert\, \theta) $ is the marginal distribution of $ Z_i $.

Next, let the marginal distribution of $ Y $ be $ f(\cdot\,\vert\, \theta) $. The relationship between the joint and the marginal distribution is

$$\begin{aligned} 
f(y_i\,\vert\, \theta) &= \sum_{j=1}^K g(y_i, Z_j=j) \\
&=\sum_{j=1}^K f(y_i\,\vert\, Z_i=j,\theta)\Pr[Z_i=j\,\vert\, \theta]\\
&=\sum_{j=1}^K f_j(y_i\,\vert\, \theta_j)\lambda_j. \end{aligned}$$

Lastly, let $ k(z \,\vert\,  y, \theta) $ be the conditional distribution of $ Z $ given $ Y $. Again, we do not observe $ Z $, but under the model specification we have

$$  k(z_i\,\vert\, y_i,\theta) = \frac{g(y_i,z_i\,\vert\, \theta)}{f(y_i\,\vert\, \theta)}  $$

Now, consider the (marginal) likelihood, obtained by summing out the unobserved latent class variable:

$$  \mathcal L({\theta}; \mathbf y) = \sum_{\mathbf z \in \mathcal Z} g(\mathbf y, \mathbf z\,\vert\, \theta)\Pr[\mathbf Z=\mathbf z \,\vert\, \theta] = f(\mathbf{y}\,\vert\, {\theta}) = \prod_{i=1}^n f(y_i\,\vert\, {\theta})  $$,

where $ \mathcal Z $ is the support of $ \mathbf Z = (Z_1,...,Z_n) $. The product is annoying, so let us take the the log of both sides, which gives the log-likelihood

$$  \log \mathcal L(\theta; \mathbf y) = \sum_{i=1}^n \log f(y_i\,\vert\, \theta)  $$

This is often referred to as the <em>incomplete-data log-likelihood</em>. If we would be able to observe the values of $ \mathbf Z $, represented by the data vector $ \mathbf z = (z_1,z_2,...,z_n) $, then we could observe the <em>complete-data log-likelihood</em>,

$$\begin{aligned}
\log \mathcal L(\theta;\mathbf y, \mathbf z) &= \sum_{i=1}^n \log g(y_i,z_i\,\vert\, \theta) \\
&=\sum_{i=1}^n \log\left(\prod_{j=1}^K\big[f_j(y_i\,\vert\, \theta_j)\lambda_j\big]^{\mathbb I(z_i=j)}\right) \\
&= \sum_{i=1}^n \sum_{j=1}^K \mathbb I(z_i=j) \log \Big(f_j(y_i\,\vert\, \theta_j)\lambda_j\Big).
\end{aligned} $$

In other words, if we would know from which class the observations come from, we could use those observations for which $ Z=j $ to estimate $ \theta_j $ and those obsevation for which $ Z=j' $ to estimate $ \theta_{j'} $, and so on. In many situations, this will make the estimation much easier. For example, suppose that we have a random sample of size $ n $ and that all the $ f_j $'s were normal densities. Then, if we would know the values of $ Z $, then we could estimate the mean and the standard deviation of each $ f_j $ with the sub-class sample mean and standard deviation. As this is not the case, we have to work with the marginal log-likelihood.

<h3>Excursion on Gibbs' Inequality</h3>

To show why the EM algorithm works, we need an important inequality known as the <em>Gibbs' Inequality</em>. Suppose $ P=(p_1,...,p_m) $ and $ Q=(q_1,...,q_m) $ are two discrete probability distributions. The Gibbs Inequality states that

$$  -\sum_{i=1}^m p_i \log p_i \le -\sum_{i=1}^m p_i \log q_i  $$

with equality if and only if $ p_i=q_i $ for all $ 1\le i\le m $, and where we take $ 0*\log(0) $ as $ \lim_{z\rightarrow 0^+} z\log(z) = 0 $. To prove the inequality, we need still another important inequality, namely

$$  1+ x \le e^x  $$

for all $ x\in \mathbb R $. This inequality is quite easy to prove. Consider two functions, $ y(x) = 1+x $ and $ z(x) = e^x $, and note that $ y(x) $ is the tangent line of $ z(x) $ at $ x=0 $, i.e. $ y'(0)=z'(0) $. As $ z(x)=e^x $ is convex, it always lies above its tangent lines. This proves that $ 1+x \le e^x $ with equality if and only if $ x=0 $.

Now, let $ I $ denote the set of all $ i $ for which $ p_i $ is non-zero. By the preceeding inequality, we have that $ \log x \le x-1 $ for all $ x\in (0,\infty) $, with equality if and only if $ x=1 $. Choosing $ x=p_i/q_i $ and multiplying both sides $ p_i $, we have

$$  p_i \log (q_i/p_i) \le p_i\left(\frac{q_i}{p_i}-1\right).  $$

Summing over all non-zero $ p_i $ and multiplying both sides by -1 gives

$$  -\sum_{i\in I} p_i\log(q_i/p_i) \ge -\sum_{i\in I}q_i + \sum_{i\in I}p_i \ge 0.  $$

But

$$  -\sum_{i\in I}p_i\log (q_i/p_i) =-\sum_{i\in I}p_i\log q_i + \sum_{i\in I}p_i\log q_i.  $$

It follows that

$$  -\sum_{i\in I}p_i \log q_i \ge - \sum_{i\in I} p_i \log p_i  $$

as desired.

<h3>Back to the EM algorithm</h3>

We are ready to show that the we can use the expectation of $ g(\mathbf y,\mathbf z\,\vert\, \theta) $ under the distribution $ k(\mathbf z\,\vert\, \mathbf y, \theta) $ to maximize the marginal likelihood, $ f(\mathbf y\,\vert\, \theta) $, which is the key of the EM algorithm. First, write the marginal likelihood as

$$  f(\mathbf y\,\vert\, \theta) = \frac{g(\mathbf y, \mathbf z\,\vert\, \theta)}{k(\mathbf z \,\vert\,  \mathbf y, \theta)}.  $$

As we are assuming that observations are independent, the marginal log-likelihood of the sample can be expressed as

$$  \mathcal L(\theta; \mathbf y)=\prod_{i=1}^n f(y_i\,\vert\, \theta) = \prod_{i=1}^n\frac{g(y_i, z_i\,\vert\, \theta)}{k(z_i \,\vert\,  y_i, \theta)}.  $$

By taking logs on both sides, we get

$$  \log\mathcal L(\theta; \mathbf y)= \log\mathcal L(\theta; \mathbf y, \mathbf z) - \log k(\mathbf z\,\vert\, \mathbf y, \theta)  $$

where $ k(\mathbf z\,\vert\, \mathbf y,\theta) = \prod_{i=1}^n k( z_i\,\vert\, y_i, \theta) $. The crucial step for the EM alorithm is to fix $ \theta $ at any feasible parameter value $ \tilde\theta $ and take expectations of both sides of the last equation with respect to the distribution $ k(\mathbf z\,\vert\, \mathbf y,\tilde\theta) $. That is,

$$  \begin{aligned} \mathbb E_{\mathbf Z\,\vert\, \mathbf y,\theta}[\log\mathcal L(\theta; \mathbf y)]&= \sum_{\mathbf z\in \mathcal Z}\log\mathcal L(\theta; \mathbf y) k(\mathbf z\,\vert\, \mathbf y, \tilde\theta)\\ 
&= \sum_{\mathbf z\in \mathcal Z}\Big[\log\mathcal L(\theta; \mathbf y, \mathbf z) - \log k(\mathbf z\,\vert\, \mathbf y, \theta)\Big] k(\mathbf z\,\vert\, \mathbf y, \tilde\theta) \\
&= \sum_{\mathbf z\in \mathcal Z}\log\mathcal L(\theta; \mathbf y, \mathbf z)k(\mathbf z\,\vert\, \mathbf y, \tilde\theta) -  \sum_{\mathbf z\in \mathcal Z}\log k(\mathbf z\,\vert\, \mathbf y, \theta)k(\mathbf z\,\vert\, \mathbf y, \tilde\theta) \end{aligned} $$

Note that $ \sum_{\mathbf z\in \mathcal Z} k(\mathbf z\,\vert\, \mathbf y,\tilde\theta) $ sums to one, as it is a proper distribution, and that $ \log\mathcal L(\theta; \mathbf y) $ does not depend on $ \mathbf Z $. It follows that

$$\begin{aligned}
\log\mathcal L(\theta; \mathbf y) &= \sum_{\mathbf z\in \mathcal Z}\log\mathcal L(\theta; \mathbf y, \mathbf z)k(\mathbf z\,\vert\, \mathbf y, \tilde\theta) - \sum_{\mathbf z\in \mathcal Z}\log k(\mathbf z\,\vert\, \mathbf y, \theta)k(\mathbf z\,\vert\, \mathbf y, \tilde\theta) \\
&=Q(\theta\,\vert\, \tilde\theta) + H(\theta\,\vert\, \tilde\theta)
\end{aligned} $$

Evaluating this expression at $ \tilde\theta $, we have $ \log \mathcal L(\tilde\theta\,\vert\, \mathbf y) = Q(\tilde\theta\,\vert\, \tilde\theta) + H(\tilde\theta\,\vert\, \tilde\theta) $ and

$$  \log\mathcal L(\theta; \mathbf y) - \log \mathcal L(\tilde\theta;\mathbf y) = Q(\theta\,\vert\, \tilde\theta) - Q(\tilde\theta\,\vert\, \tilde\theta) + H(\theta\,\vert\, \tilde\theta) - H(\tilde\theta\,\vert\, \tilde\theta) $$

for any other $ \theta \in \Theta $, where $ \Theta $ is the parameter space. However, by Gibbs' Inequality,

$$  H(\theta\,\vert\, \tilde\theta)= - \sum_{\mathbf z\in \mathcal Z}\log k(\mathbf z\,\vert\, \mathbf y, \theta)k(\mathbf z\,\vert\, \mathbf y, \tilde\theta) \ge - \sum_{\mathbf z\in \mathcal Z} \log k(\mathbf z\,\vert\, \mathbf y, \tilde\theta)k(\mathbf z\,\vert\, \mathbf y, \tilde\theta) = H(\tilde\theta\,\vert\, \tilde\theta).  $$

Therefore,

$$  \log \mathcal L(\theta; \mathbf y)-\log \mathcal L(\tilde\theta; \mathbf y) \ge Q(\theta\,\vert\, \tilde\theta) - Q(\tilde\theta\,\vert\, \tilde\theta).  $$

The meaning of the last inequality is as follows. Suppose that we have a guess of the parameter value that maximizes the (marginal) likelihood of the data, which we denote by $ \tilde\theta $. Then, choosing another parmeter value $ \theta \in \Theta $ that increases $ Q(\theta\,\vert\, \tilde\theta) $ beyond $ Q(\tilde\theta\,\vert\, \tilde\theta) $ must lead to an improvement in $ \log \mathcal L (\theta;\mathbf y) $ from $ \log\mathcal L(\tilde\theta;\mathbf y) $. In other words, by maximizing

$$  Q(\theta\,\vert\, \tilde\theta) = \sum_{\mathbf z\in \mathcal Z} \log\mathcal L(\theta; \mathbf y, \mathbf z) k(\mathbf z \,\vert\,  \tilde\theta, \mathbf y) = \mathbb E_{\mathbf Z\,\vert\, \mathbf y, \tilde\theta}[\log \mathcal L(\theta; \mathbf y, \mathbf z)],  $$

i.e., the expected complete data log-likelihood under $ k(\cdot \,\vert\, \mathbf y,\tilde\theta) $, we are able to maximize the marginal likelihood, $ \mathcal L (\theta; \mathbf y) = f(\mathbf y\,\vert\, \theta) $.

<h4>Implementation of the Algorithm</h4>

Given these results, we can obtain maximum likelihood estimates as follows. We start with a set of initial estimates of the prameter $ \hat\theta^{(t)} $.

<ol>
<li>Given the current estimates of the parameter vector $ \hat{\theta}^{(t)} $, calculate the expected complete-data likelihood (where the expectation is with respect to the conditional distribution of $ \mathbf Z $), $ Q(\theta\,\vert\, \hat{\theta}^{(t)})=\mathbb E_{\mathbf Z\,\vert\, \mathbf y, \hat{\theta}^{(t)}}[\log \mathcal L(\theta; \mathbf y, \mathbf z)] $ (E-Step).</li>
<li>Maximize $ Q(\theta\,\vert\, \hat{\theta}^{(t)}) $ with respect to $ \theta $ to obtain new parameter estimates, $ \hat{\theta}^{(t+1)} $ (the M-step)</li>
<li>Repeat step 1. and 2. until convergence.</li>
</ol>

All this seems to be somewhat abstract. So, we turn to a concrete example.

<h3>A Simple Simulation</h3>

Consider a mixture of two normal components, where we set $ \mu_1=-.7, \sigma_1=.2 $ and $ \mu_2=.5,\sigma_2=.6 $ and let $ \lambda_1=.7 $. Let us generate a sample of size $ n=500 $.

{% highlight r %}
# load packages
library('dplyr')
library('ggplot2')
library('grid')
library('gridExtra')
library('viridis')

# set seed
set.seed(1984)

# number of observations
n <- 500

# parameters
mu <- c(-.7, .5)
sigma <- c(.3, .6)
lambda <- .7

# latent classe sizes
s.class.1 <- n * lambda
s.class.2 <- n * (1 - lambda)
# latent classes
z <- c(rep(1, n * lambda), rep(0, n * (1 - lambda)))
# outcome
y <- ifelse(z==1,
    rnorm(s.class.1, mu[1], sigma[1]),
    rnorm(s.class.2, mu[2], sigma[2]))
{% endhighlight %}

Let us have a look at the data.

{% highlight r %}
p1 <- ggplot(data.frame(y = y, z = z), aes(x = y)) +
     theme_bw() +
     geom_histogram(aes(y = ..density..),
                    fill = 'black',
                    alpha = .8,
                    color = 'white',
                    bins = 40) +
     ggtitle('Aggregate Density') +
     labs(x = '', y = '')

p2 <- ggplot(data.frame(y = y,z = z), 
             aes(x = y, fill = factor(z))) +
     theme_bw() +
     geom_histogram(aes(y = ..density..),
                    position = 'identity',
                    col = 'white',
                    bins = 40) +
     scale_fill_viridis(name = 'Latent Classes',
                        discrete = T,
                        alpha = .6,
                        begin = 0,
                        end = .5) +
     ggtitle('Density by Latent Classes') +
     labs(x = '', y = '')

grid.arrange(p1, p2, nrow = 1)
{% endhighlight %}

<center>
<img src="/assets/img/mixture1.jpg" width="700" height="500"/>
</center>

The marginal log-likelihood of the data is

$$  \log \mathcal L(\mu_1,\mu_2,\sigma_1,\sigma_2,\lambda ; \mathbf y) = \sum_{i=1}^n \log\Big(\lambda\phi(y_i\,\vert\,  \mu_1,\sigma_1) + (1-\lambda)\phi(y_i\,\vert\, \mu_2,\sigma_2)\Big)  $$

<h4>Direct Maximization of Marginal Likelihood</h4>

First, let us try to maximize this function directly. The log-likelihood can be coded as

{% highlight r %}
ll <- function(pars) {
     # parameters entered as lambda, mu1, sigma1, mu2, sigma2

     # log-likelihood contribution of each observations
     ll.i <- log(pars[1] * dnorm(y, pars[2], pars[3]) +
                (1 - pars[1]) * dnorm(y, pars[4], pars[5]))

    # log-likelihood of sample
     sum(ll.i)
}

inits <- c(.5, -.2, .2, .3, .1)
fit.1 <- optim(inits, #initial values
               ll, #likelihood function
               control=list(fnscale = -1, # maximize funtion
               maxit = 1000), # maximum iterations
               method = 'L-BFGS-B', # maximization method (quasi-Newton)
               lower=c(0, -Inf, 0, -Inf,0), # lower bounds for parameters
               upper=c(1, Inf, Inf, Inf, Inf), # upper bounds for parameters
               hessian = T) # return Hessian

# print exit code
fit.1$convergence
{% endhighlight %}

<pre><code>## [1] 0
</code></pre>

{% highlight r %}
# Calculate asymp. standard errors
obs.info <- -solve(fit.1$hessian) %>% diag %>% sqrt
# parameter estimates and true values
matrix(rbind(c(lambda, mu[1], sigma[1], mu[2], sigma[2]),
               fit.1$par,
               obs.info), nrow=3,
               dimnames=list(c('True Value', 'Estimate', 'Asy. S.E.'),
               c('lambda', 'mu1', 'sigma1', 'mu2', 'sigma2')))
{% endhighlight %}

<pre><code>##                lambda         mu1     sigma1        mu2     sigma2
## True Value 0.70000000 -0.70000000 0.30000000 0.50000000 0.60000000
## Estimate   0.68078942 -0.73337042 0.26903817 0.49559204 0.59128636
## Asy. S.E.  0.03472845  0.01858723 0.01398739 0.09742945 0.06454193
</code></pre>

{% highlight r %}
# log-likelihood
fit.1$value
{% endhighlight %}

<pre><code>## [1] -413.3636
</code></pre>

Close! Next, let us try out the EM Algorithm.

<h3>EM Algorithm</h3>

<h4>The E-step</h4>

For the E-step, we have to calculate the expectation $ Q(\theta\,\vert\, \hat\theta) = \mathbb E_{\mathbf Z\,\vert\, \hat\theta,\mathbf y}[\mathcal L(\theta; \mathbf y, \mathbf z)] $. For notational convenience let $ \hat\theta $ be our current estimate of the parameter vector. First consider the conditional distribution of $ \mathbf Z $, given $ \mathbf y $ and $ \hat\theta $. By Bayes' Rule, the conditional probability of $ Z_i=j $ is

$$  \Pr[Z_i=j\,\vert\, Y_i=y_i,\hat\theta] = \frac{\Pr[Z_i=j \cap Y_i=y_i\,\vert\, \hat\theta]}{\Pr[Y_i=y_i\,\vert\, \hat\theta]}=\frac{\Pr[Y_i=y_i\,\vert\, Z_i=j, \hat\theta]\Pr[Z_i=j\,\vert\, \hat\theta]}{\sum_{j=1}^K\Pr[Y_i=y_i\,\vert\, Z_i=j,\hat\theta]\Pr[Z_i=j\,\vert\, \hat\theta]}.  $$

The numerator of this expression is $ \text{Normal}(y_i\,\vert\, \hat\theta_j)\hat\lambda_j $ and the denominator is $ \sum_{j=1}^K\text{Normal}(y_i\,\vert\, \hat\theta_j)\hat\lambda_j = \text{Normal}(y_i\,\vert\, \hat\theta_1)\hat\lambda_1 + \text{Normal}(y_i\,\vert\, \hat\theta_2)(1-\hat\lambda) $ as we have only two latent classes. Thus,

$$  \Pr[Z_i=j \,\vert\, Y_i=y_i, \hat\theta] = [\mathcal(y_i\,\vert\, \hat\theta_j)\hat\lambda_j]/[\text{Normal}(y_i\,\vert\, \hat\theta_1)\hat\lambda_1 + \text{Normal}(y_i\,\vert\, \hat\theta_2)(1-\hat\lambda)] $$.

Now, consider the complete-data likelihood. Assuming that we have observed $ \mathbf z $, the log-likelihood can be formulated as

$$  \log \mathcal L(\theta; \mathbf y, \mathbf z) = g(\mathbf y,\mathbf z\,\vert\, \hat\theta) = \sum_{i=1}^n \sum_{j=1}^K\mathbb I(z_i=j)\log \Big(\lambda_j \text{Normal}(y_i\,\vert\, \theta_j)\Big).  $$

Thus, we have

$$ \begin{aligned}
Q(\theta\,\vert\, \hat\theta) &= \mathbb E_{\mathbf Z\,\vert\, \mathbf y,\hat\theta}[\log\mathcal L(\theta; \mathbf y, \mathbf z)] \\ &= \mathbb E_{\mathbf Z\,\vert\, \mathbf y,\hat\theta}\left(\sum_{i=1}^n\sum_{j=1}^K\mathbb I(z_i=j)\log[\lambda_j \text{Normal}(y_i\,\vert\, \theta_j)]\right) \\
&=\sum_{i=1}^n\sum_{j=1}^K \mathbb E_{\mathbf Z\,\vert\, \hat\theta,\mathbf y}[\mathbb I(z_i=j)\log \text{Normal}(y_i\,\vert\, \theta_j)] \\
&=\sum_{i=1}^n\sum_{j=1}^K \Pr[Z_i=j\,\vert\, Y_i=y_i,\hat\theta]\log[\lambda_j \text{Normal}(y_i\,\vert\, \theta_j)] \\
&=\sum_{i=1}^n\sum_{j=1}^K \frac{\text{Normal} (y_i\,\vert\, \hat\theta_j)\hat\lambda_j}{\text{Normal}(y_i\,\vert\, \hat\theta_1)\hat\lambda_1 + \text{Normal}(y_i\,\vert\, \hat\theta_2)(1-\hat\lambda)}\log [\lambda_j\text{Normal}(y_i\,\vert\, \theta_j)]
\\ &= \sum_{i=1}^n \sum_{j=1}^K \hat w_{ij}\Big[\log\lambda_j+ \log \text{Normal}(y_i\,\vert\, \theta_j)\Big]
\end{aligned} $$

where $ \hat w_{ij} = \Pr[Z_i=j\,\vert\, y_i,\hat\theta] $. This is the function that we want to maximize (with respect to $ \theta $) in the M-step.

<h4>The M-Step</h4>

Taking a second look into $ Q(\theta\,\vert\, \hat\theta) $ and expanding the inner summation, we have

$$ \begin{aligned}
Q(\theta\,\vert\, \hat\theta) &= \sum_{i=1}^n \Big(\hat w_{i1}[\log \lambda + \log \text{Normal}(y_i\,\vert\, \theta_1) + \hat w_{i2}[\log (1-\lambda) + \log \text{Normal}(y_i\,\vert\, \theta_2)]\Big)
\end{aligned} $$

As $ \hat w_{ij} $ is assumed to be known, and the sets of parameters $ \lambda, \theta_1,\theta_2 $ enter $ Q(\theta\,\vert\, \hat\theta) $ linearly, we can maximize the function by maximizing each set of parameters separately.

Thus, for $ \lambda $ we have

$$  \lambda^* = \text{argmax}_{\lambda} Q(\lambda\,\vert\, \hat\theta) = \text{argmax}_{\lambda} \sum_{i=1}^n \Big[\hat w_{i1}\log \lambda + \hat w_{i2}\log (1-\lambda)\Big]  $$

As the maximand is a differentiable and concave function in $ \lambda $, we might solve the maximization by seting the first derivative to zero:

$$  \frac{\partial Q}{\partial \lambda}= \sum_{i=1}^n \left(\frac{w_{i1}}{\lambda} - \frac{w_{i2}}{1-\lambda}\right)=0  $$

which leads to the solution

$$  \lambda^* = \frac{1}{n}\sum_{i=1}^n w_{i1}.  $$

Next, consider the vector $ \theta_1 = (\mu_1,\sigma_1) $. Dropping unnecessary terms, we want to find

$$  \theta_1^* = \text{argmax}_{\theta_1}Q(\theta_1\,\vert\, \hat\theta) = \text{argmax}_{\theta_1} \sum_{i=1}^n \hat w_{i1}\left[-\frac{1}{2}\log\sigma_1^2 - \frac{1}{2\sigma_1^2}\left(y_i-\mu_1\right)^2\right].  $$

Again, we know in advance that this function is differentiable and concave. Let $ \theta_1=(\mu_1,\sigma_1^2) $. The gradient is

$$ \frac{\partial Q}{\partial \theta_1} = \begin{bmatrix}
\frac{\partial Q}{\partial \mu_1} \\
\frac{\partial Q}{\partial \sigma_1^2}
\end{bmatrix}
= \begin{bmatrix} \frac{1}{\sigma^2}\sum_{i=1}^n w_{i1}(y_i-\mu_1) \\
-\frac{1}{2\sigma_1^2}\sum_{i=1}^n w_{i1} + \frac{1}{\sigma_1^4}\sum_{i=1}^n w_{i1}(y_1-\mu_1)^2 \end{bmatrix} $$

Setting it to the zero and solving for $ \mu_1 $ gives

$$  \mu_1^* = \frac{\sum_{i=1}^n w_{i1}y_i}{\sum_{i=1}^n w_{i1}}.  $$

Plugging this value into the second equation and solving for $ \sigma_1^2 $ gives

$$  {\sigma_1^2}^* = \frac{\sum_{i=1}^n w_{i1} (y_i-\mu_1^*)^2}{\sum_{i=1}^n w_{i1}}.  $$

Lastly, by symmetry, we have

$$  \mu_2^* = \frac{\sum_{i=1}^n w_{i2}y_i}{\sum_{i=1}^n w_{i2}} \quad \text{and} \quad {\sigma_2^2}^* = \frac{\sum_{i=1}^n w_{i2} (y_i-\mu_2^*)^2}{\sum_{i=1}^n w_{i2}}.  $$

<h3>Application of the EM algorithm</h3>

To code the algorithm, we need first a function that calculates the weights $ w_{ij} $ which will be needed to maximize $ Q(\theta\,\vert\, \hat\theta) $

{% highlight r %}
my.em.weights <- function(old.pars,y=y) {
     # numerator for j = 1
     num.1 <- old.pars[1] * dnorm(y, old.pars[2], old.pars[3])
     # numerator for j = 2
     num.2 <- (1 - old.pars[1]) * dnorm(y, old.pars[4], old.pars[5])
     # common denominator
     denom <- num.1 + num.2
     # generate weights w_{ij}
     cbind(num.1 / denom, num.2 / denom)
}
{% endhighlight %}

The maximization step can be coded as

{% highlight r %}
my.mstep <- function(old.pars, w_ij, y = y) {
     # sum of weights
     ww <- colSums(w_ij)

     # lambda
     lambda.star <- ww[1] / n
     # mu
     mu.star <- t(w_ij) %*% y / ww
     # sigma
     sigma.star.1 <- sqrt(sum(w_ij[,1] * (y - mu.star[1])^2) / ww[1])
     sigma.star.2 <- sqrt(sum(w_ij[,2] * (y - mu.star[2])^2) / ww[2])

     c(lambda.star, mu.star[1],
     sigma.star.1, mu.star[2],
     sigma.star.2)
}
{% endhighlight %}

Okay. Let's try it out!

{% highlight r %}
# marginal log-likelihood
m.ll <- function(new.par) {
     ll.i <- new.par[1] * dnorm(y, new.par[2], new.par[3]) +
             (1 - new.par[1]) * dnorm(y, new.par[4], new.par[5])
     sum(log(ll.i))
}

tol.level <- .00001
new.par <- inits
e <- Inf
iter <- 0 while(e > tol.level) {
     iter <- iter + 1
     old.par <- new.par

     # calculate weights, E-step
     w_ij <- my.em.weights(old.par, y = y)
     # maximize Q(theta | theta.old), M-step
     new.par <- my.mstep(old.par, w_ij, y = y)

     # mesages (suppressed, too much output)
     # message(paste0('Iteration ',
     # iter,
     # ': log-likelihood =',
     # m.ll(new.par)
     # )
     # )

     e <- max(abs(old.par - new.par))

}
em.par <- new.par

# estimates
matrix(rbind(c(lambda, mu[1], sigma[1], mu[2], sigma[2]),
       em.par
       ), nrow=2,
       dimnames=list(c('True Value', 'Estimate'),
          c('lambda', 'mu1', 'sigma1', 'mu2', 'sigma2')
          )
      )
{% endhighlight %}

<pre><code> ## lambda mu1 sigma1 mu2 sigma2
 ## True Value 0.7000000 -0.7000000 0.3000000 0.5000000 0.6000000
 ## Estimate 0.6808127 -0.7333609 0.2690429 0.4956679 0.5912308
</code></pre>

{% highlight r %}
# log-likelihood
m.ll(new.par)
{% endhighlight %}

<pre><code> ## [1] -413.3636
</code></pre>

Again, very close! As the last step, we need the calculate the standard
errors of the estimates. It is here that we encounter one of the
major shortcomming of the EM algorithm. Differently from Newton-Raphson
type of algorithms, the EM algorithm does not calculate the Hessian matrix. And thus, there are no accompanying estimates for the standard errors. An easy, but computationally heavy, way around this problem is to use the parametric bootstrap.

But before we go there, let us check whether we got the correct results by using the "mixtools" package.

{% highlight r %}
library(mixtools)
mix.fit <- normalmixEM(y,
     lambda = inits[1],
     mu = c(inits[c(2, 4)]),
     sigma=c(inits[c(3, 5)]))
     
summary(mix.fit)
{% endhighlight %}

<pre><code>## summary of normalmixEM object:
##           comp 1   comp 2
## lambda  0.680792 0.319208
## mu     -0.733368 0.495607
## sigma   0.269037 0.591270
## loglik at estimate:  -413.3636
</code></pre>

Almost identical results with our own function, which is expected as
both use the same algorithm. Now, let's calculate standard errors. The
parametric bootstrap method might be explained as follows (we will omit
the discussion of why it works, but the procedure itself gives the
intuition why it should):

<ol>
<li>We fit a model obtain an estimate, $ \hat\theta $, of the parameter vector $ \theta $.</li>
<li>Using the estimate, we simulate a dataset that is implied by the model and the parameters in $ \hat\theta $. For the application at hand, this means that we generate a $ n $-length vector of class memberships $ \mathbf z $ according $ z_i \sim \text{Categorical}(\lambda,1-\lambda) $. Thereafter, we sample $ y_i $ from $ \text{Normal}(\cdot\,\vert\, \mu_1,\sigma_1) $ if $ z_i=1 $ and, if $ z_i=2 $, from $ \text{Normal}(\cdot\,\vert\, \mu_1,\sigma_2) $. This will generate a bootrstrap sample with $ n $ observations.</li>
<li>We fit the EM algorithm on the bootstrapped sample to obtain a new parameter vector $ \tilde\theta $, which we store. When doing so, we have to check whether any "label switching" has occured, by which we mean the phenomenon that class 1 in labeled as 2 and vice versa (actually, the mixture model is an "unidentified" model as the likelihood of the model would be the same regardless of how we shuffle the labels.) If any label switching has occured, we change the labels of the classes so that they match to our original results.</li>
<li>We repeat steps 2. and 3. $ B $ times, which gives us an approximation to the sampling distribution of the parameter vector $ \hat\theta $.</li>
<li>We use the bootrstrapped distribution to calculate the standard errors, which will be approximated by the standard deviation of the distribution.</li>
</ol>

{% highlight r %}
# parametric bootstrap function
boot.em <- function(em.par) {

     # sample latent class
     z.b <- rbinom(n, 1, 1-em.par[1]) + 1
     # sample outcome
     y.b <- sapply(z.b, function(a) rnorm(1, em.par[2*a], em.par[2*a+1]))
     # run EM
     e <- Inf
     new.par <- em.par 
     
     while(e > tol.level) {

         old.par <- new.par

         # calculate weights, E-step
         w_ij.b <- my.em.weights(old.par,y = y.b)
         # maximize Q(theta\,\vert\, theta.old), M-step
         new.par <- my.mstep(old.par, w_ij = w_ij.b, y = y.b)

         e <- max(abs(old.par-new.par)) 
    
    } 
    
    # check for label switching (with lambda)     
    if (new.par[1] > .5) {
         return(new.par)
    } else {
          new.par <- c(1-new.par[1],
               new.par[3],
               new.par[5],
               new.par[2],
               new.par[4])
          return(new.par)
    }
}

# number of samples
B <- 200

# empty object to store results
b.mat <- matrix(NA, nrow=B, ncol = length(em.par))

# run parametric bootrstrap
for (w in 1:B) {
     b.mat[w,] <- boot.em(em.par)
}

# get column-wise standard deviation
em.boot.se <- apply(b.mat,2,sd)
{% endhighlight %}

Again, let us check our results against the results from mixtools.

{% highlight r %}
b.samps <- mixtools::boot.se(mix.fit, B = B)

matrix(rbind(c(lambda, mu[1], sigma[1], mu[2], sigma[2]),
          new.par,
          c(b.samps$latex lambda.se[1],
            b.samps$mu.se[1],
            b.samps$latex sigma.se[1],
            b.samps$mu.se[2],
            b.samps$sigma.se[2]),
            em.boot.se
          ), nrow = 4,
          dimnames=list(
            c('True Value', 'Estimate',
              'mixtools S.E.', 'my boot. S.E.'),
          c('lambda', 'mu1', 'sigma1', 'mu2', 'sigma2')
          )
      )

{% endhighlight %}

<pre><code>##                   lambda         mu1     sigma1        mu2     sigma2
## True Value    0.70000000 -0.70000000 0.30000000 0.50000000 0.60000000
## Estimate      0.68081274 -0.73336087 0.26904292 0.49566789 0.59123079
## mixtools S.E. 0.03588050  0.01988284 0.01484522 0.10088403 0.05581424
## my boot. S.E. 0.03498862  0.02117202 0.01712347 0.09933093 0.06340458
</code></pre>

Close!

<hr />

<h3>Update on Standard Errors</h3>

It just came to my mind that a quite straight-forward way to estimate the standard errors for the model in the post is to use the so-called BHHH estimator. To understand the BHHH estimator, we have to return to the Fisher Information Equality. Define the score function as

$$  u(\theta) = \frac{\partial \log \mathcal L(\theta; \mathbf y)}{\partial \theta}= \sum_{i=1}^n \frac{\partial \log f(y_i\,\vert\, \theta)}{\partial \theta}=\sum_{i=1}^n u_i(\theta) $$

and let

$$  H(\theta) = \frac{\partial^2 \log \mathcal L(\theta;\mathbf y)}{\partial \theta \partial \theta'} = \sum_{i=1}^n \frac{\partial^2 \log f(y_i\,\vert\, \theta)}{\partial \theta\partial \theta'}=\sum_{i=1}^n H_i(\theta) $$

be the Hessian matrix. The Information Equality states that the negative of the expected Hessian is equal to the expected outer product of the score function, i.e.,

$$  -\mathbb E[H(\theta)] = \mathbb E[u(\theta)u(\theta)']  $$

The proof relies on assumption that the so-called regularity conditions hold. For notational simplicity, let $ L=\mathcal L(\theta;\mathbf y) $ and observe that

$$  \mathbb E\left[\frac{\partial^2 \log L }{\partial\theta\partial\theta'}\right] = \mathbb E\left[\frac{\partial }{\partial\theta}\left(\frac{1}{L} \frac{\partial  L}{\partial\theta'}\right)\right]=\mathbb E\left[-\frac{1}{L^2}\left(\frac{\partial L}{\partial\theta}\frac{\partial L}{\partial \theta'}\right)+ \frac{1}{L}\frac{\partial^2 L}{\partial \theta \partial \theta'}\right]. $$

The first term in the expectation is equal to

$$  -\mathbb E\left[\left(\frac{\partial \log L}{\partial \theta}\right)\left(\frac{\partial \log L}{\partial \theta'}\right)\right] = -\mathbb E[u(\theta)u(\theta)']. $$

Thus, if we show that $ \mathbb E[(1/L)(\partial^2 L / \partial \theta\partial \theta')]=0 $, it is proven that $ \mathbb E[u_0u_0'] = -\mathbb E[H_0] $. So, let us focus on the second term. By writing out the expectation, and noting that $ L=f(\mathbf y\,\vert\, \theta) $, we obtain

$$ \mathbb E\left[\frac{1}{L}\frac{\partial^2 L}{\partial \theta\partial \theta}\right] = \int \frac{1}{L}\frac{\partial^2 L}{\partial \theta\partial \theta}L d\mathbf y=\int \frac{\partial^2 L}{\partial \theta\partial \theta}d\mathbf y = \frac{\partial^2}{\partial \theta\partial \theta}\int L d\mathbf y=\frac{\partial^2}{\partial \theta\partial \theta}(1)=0, $$

where it is understood that we are integrating over the support of $ \mathbf y $. Note that we have interchanged the two limiting process of integration and differentition twice in the third step, which is valid as long as the likelihood is sufficiently smooth. Also, we have used the fact that $ \int L = \int f(\mathbf y\,\vert\, \theta)d\mathbf y = 1 $. This proves the Fisher Information Equality.

Now, recall that $ -\mathbb E[H(\theta_0)^{-1}] $ is the covariance matrix of the MLE, where $ \theta_0 $ is the "true" parameter vector. The matrix of second partial derivatives is often a complicated function of the data, so that a closed form solution is difficult to obtain. Therefore, the expected Hessian is often estimated by $ H(\hat\theta) $, called the observed information matrix. However, with mixture models, analytical derivatives are cumbersome to derive or hard to program. In this case, we might use the Information Equality and estimate the expected Hessian by $ u(\hat\theta)u(\hat\theta)' $.

As we are assuming a random sample, we have

$$ -\mathbb E[H(\theta)] = \mathbb E[u(\theta) u(\theta)'] = \mathbb E\left[\sum_{i=1}^n u_i \sum_{j=1}^n u_j'\right] = \mathbb E\left[\sum_{i=1}^n u_iu_i'\right] $$

as for all $ i\ne j, \mathbb E[u_iu_j'] = \mathbb E[u_i]\mathbb E[u_j]=0 $. Thus, by the consistency of the MLE and the continuous mapping theorem, we have

$$  \text{plim}_{n\rightarrow \infty} \sum_{i=1}^n u_i(\hat\theta)u_i(\hat\theta)' = -\mathbb E\left[H(\theta_0)\right], $$

which shows that we might estimate the covariance matrix with $ \left[\sum_{i=1}^n u_iu_i'\right]^{-1} $ evaluated at the MLE.

<h4>Calculating Standard Errors using the BHHH Estimator</h4>

Now, recall that the marginal log-likelihood of the model is

$$  \log \mathcal L(\theta; \mathbf y) = \sum_{i=1}^n \log\left(\sum_{j=1}^K\lambda_j f(y_i\,\vert\, \theta_j)\right). $$

The first-derivative with respect to the parameters is thus,

$$  \frac{\partial \log \mathcal L(\theta;\mathbf y)}{\partial \lambda} = \sum_{i=1}^n \frac{f(y_i\,\vert\, \mu_1,\sigma_1) - f(y_i\,\vert\, \mu_2,\sigma_2)}{\lambda f(y_i\,\vert\, \mu_1,\sigma_1) + (1-\lambda) f(y_1\,\vert\, \mu_2,\sigma_2)} $$

$$ \frac{\partial \log \mathcal L(\theta;\mathbf y)}{\partial \mu_1} = \sum_{i=1}^n \frac{\lambda\left(\frac{y_i-\mu_1}{\sigma_1^2}\right)f(y_i\,\vert\, \mu_1,\sigma_1)}{\lambda f(y_i\,\vert\, \mu_1,\sigma_1) + (1-\lambda) f(y_1\,\vert\, \mu_2,\sigma_2)} $$

$$  \frac{\partial \log \mathcal L(\theta;\mathbf y)}{\partial \sigma_1^2}=\sum_{i=1}^n \frac{\lambda\left[\frac{(y_i-\mu_1)^2-2\sigma_1^2}{2\sigma_1^4}\right]f(y_i\,\vert\, \mu_1,\sigma_1)}{\lambda f(y_i\,\vert\, \mu_1,\sigma_1) + (1-\lambda) f(y_1\,\vert\, \mu_2,\sigma_2)}; $$

the partial derivatives with respect to $ \mu_2 $ and $ \sigma_2 $ are derived analogously as

$$  \frac{\partial \log \mathcal L(\theta;\mathbf y)}{\partial \mu_2} = \sum_{i=1}^n \frac{(1-\lambda)\left(\frac{y_i-\mu_2}{\sigma_2^2}\right)f(y_i\,\vert\, \mu_2,\sigma_2)}{\lambda f(y_i\,\vert\, \mu_1,\sigma_1) + (1-\lambda) f(y_1\,\vert\, \mu_2,\sigma_2)} $$

and

$$  \frac{\partial \log \mathcal L(\theta;\mathbf y)}{\partial \sigma_2^2}=\sum_{i=1}^n \frac{(1-\lambda)\left[\frac{(y_i-\mu_2)^2-2\sigma_2^2}{2\sigma_2^4}\right]f(y_i\,\vert\, \mu_2,\sigma_2)}{\lambda f(y_i\,\vert\, \mu_1,\sigma_1) + (1-\lambda) f(y_1\,\vert\, \mu_2,\sigma_2)}. $$

For notational simplicity we have so far written $ \sum_i u_i u_i' $. As we have 5 parameters in the model, this is a sum of $ n $ $ 5\times 5 $ matrices. Yet, this sum can be also expressed as

$$ \sum_{i=1}^n u_iu_i' = U'U, $$

where $ U=[u_1,u_2,...,u_n]' $ and, thus, $ U $ is a $ n\times 5 $ matrix created by stacking the score evaluated at the $ n $ data points, in row-vector form, over one another. That is, the $ i $th row of $ U $ is $ u_i'=[u_{i,\lambda}, u_{i,\mu_1},...,u_{i,\sigma_2}] $.

We use the following code in R

{% highlight r %}
my.em.score <- function(pars,x) {

   lambda <- pars[1]
   mu1 <- pars[2]
   sigma1 <- pars[3]
   mu2 <- pars[4]
   sigma2 <- pars[5]

   # normal density mu1,sigma1
   norm.1 <- dnorm(x,mu1,sigma1)
   norm.2 <- dnorm(x,mu2,sigma2)

   # common denominator
   c.denom <- 1 / (lambda * norm.1 + (1 - lambda) * norm.2)

   # score vector evaluated at mle (row-vector form, each obs.)
   grad.i <- matrix(NA, nrow = length(x), ncol = length(pars))

   # lambda
   grad.i[,1] <-c.denom * (norm.1 - norm.2)
   # mu1
   grad.i[,2] <- c.denom * lambda * 
                 ((x - mu1) / sigma1^2) * norm.1
   # sigma1
   grad.i[,3] <- c.denom*lambda*
                (((x - mu1)^2 - 2 * sigma1^2) / 
                (2 * sigma1^4)) *norm.1
   # mu2
   grad.i[,4] <- c.denom * (1 - lambda)*
                ((x - mu2) / sigma2^2) * norm.2
   # sigma2
   grad.i[,5] <- c.denom*(1 - lambda)*
                (((x - mu2)^2 - 2 * sigma2^2) / 
                (2 * sigma2^4)) * norm.2

   # return U'U
   return(t(grad.i) %*% grad.i)
}

# get U'U
UU <- my.em.score(pars = new.par, x = y)

# standard errors
bhhh.se <- solve(UU)%>%diag%>%sqrt

# compare with other results
matrix(rbind(c(lambda, mu[1], sigma[1], mu[2], sigma[2]),
             new.par,
             c(b.samps$lambda.se[1],
               b.samps$mu.se[1],
               b.samps$sigma.se[1],
               b.samps$mu.se[2],
               b.samps$sigma.se[2]),
             em.boot.se,
             bhhh.se,
             obs.info
             ), nrow=6,
       dimnames=list(c('True Value', 'Estimate',
                       'mix boot. S.E.',
                       'my boot. S.E.',
                       'BHHH S.E.',
                       'Obs. Info S.E.'),
                     c('lambda', 'mu1', 'sigma1',
                       'mu2', 'sigma2')
                    )
      )

{% endhighlight %}

<pre><code> ##                   lambda         mu1      sigma1        mu2     sigma2
 ## True Value     0.68081274 -0.70000000 0.300000000 0.50000000 0.60000000
 ## Estimate       0.68081274 -0.73336087 0.269042916 0.49566789 0.59123079
 ## mix boot. S.E. 0.03588050  0.01988284 0.014845223 0.10088403 0.05581424
 ## my boot. S.E.  0.03498862  0.02117202 0.017123467 0.09933093 0.06340458
 ## BHHH S.E.      0.06283449  0.01888470 0.007282441 0.10233306 0.07696278
 ## Obs. Info S.E. 0.03472845  0.01858723 0.013987388 0.09742945 0.06454193
</code></pre>

Note that the BHHH estimate of the standard error of $ \sigma_1 $ is almost half the size of other estimates. In theory, these difference should disappear as $ n $ grows larger. In hindsight, however, the BHHH estimator might not be very appealing in the context of mixture models estimated by EM. One of the main merits of the EM algorithm is that we do not need to compute gradients (or Hessians) for the optimization. This is a valuable feature, especially in situations where the number of estimated parameters is large. The BHHH estimator is, however, based on gradients of the log-likelihood function, which can be quite difficult to analytically derive in many situations.
